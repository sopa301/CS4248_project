{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates images for processing by the VLM\n",
    "# Prereq for running this script: you need the datasets for the respective generators \n",
    "# download and unzip google's emoji dataset here:\n",
    "# https://github.com/googlefonts/noto-emoji/\n",
    "# \n",
    "# download and unzip twitter's emoji dataset here:\n",
    "# https://github.com/jdecked/twemoji/\n",
    "#\n",
    "# Yes, I am not a data person.\n",
    "\n",
    "OUTPUT_SIZE = 224 # it's a square image\n",
    "MAX_EMOJIS_PER_ROW = 3\n",
    "MAX_EMOJI_SEQUENCE_LENGTH = MAX_EMOJIS_PER_ROW * MAX_EMOJIS_PER_ROW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import grapheme\n",
    "\n",
    "elco_df = pd.read_csv('elco.csv')\n",
    "\n",
    "# count the lengths and graph it\n",
    "elco_df['length'] = elco_df['EM'].apply(lambda x: len(list(grapheme.graphemes(x))))\n",
    "elco_df['length'].hist(bins=elco_df['length'].max())\n",
    "elco_df['length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of lengths > 9\n",
    "elco_df[elco_df['length'] > 9].shape[0]\n",
    "elco_df[elco_df['length'] > 4].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common utils\n",
    "import os\n",
    "import cv2\n",
    "import grapheme\n",
    "import numpy as np\n",
    "import shutil\n",
    "np.random.seed(0)\n",
    "\n",
    "def get_png_image_from_local_repo(emoji, image_dir, filename_getter_fn, extension=\"png\"):\n",
    "  filename = filename_getter_fn(emoji, extension)\n",
    "  path = f'{image_dir}/{filename}'\n",
    "  if not os.path.exists(path):\n",
    "    print(f\"Couldn't find image for {emoji} at {path}\")\n",
    "    return None\n",
    "  img = cv2.imread(path, cv2.IMREAD_COLOR)  # Use only RGB channels\n",
    "  img_new_size = OUTPUT_SIZE // MAX_EMOJIS_PER_ROW\n",
    "  img = cv2.resize(img, (img_new_size, img_new_size))\n",
    "  return img\n",
    "\n",
    "def generate_dataset(local_image_dir, output_folder, filename_generator_fn, randomise=False):\n",
    "  if os.path.exists(output_folder):\n",
    "    # delete the directory\n",
    "    shutil.rmtree(output_folder)\n",
    "\n",
    "  os.makedirs(output_folder)\n",
    "\n",
    "  undone = 0\n",
    "\n",
    "  i = -1\n",
    "  for text in elco_df['EM']:\n",
    "    i += 1\n",
    "    units = list(grapheme.graphemes(text))\n",
    "    units = [unit for unit in units if unit != ',' and unit != ' '] # don't ask me why they're there\n",
    "    if len(units) > MAX_EMOJI_SEQUENCE_LENGTH:\n",
    "      print(f\"Skipping {text} because it's too long\")\n",
    "      undone += 1\n",
    "      continue\n",
    "    \n",
    "    output_filename = f'{output_folder}/{i}.png'\n",
    "    \n",
    "    x = 0\n",
    "    y = 0\n",
    "    # Make a white image of size OUTPUT_SIZE x OUTPUT_SIZE\n",
    "    canvas = np.zeros((OUTPUT_SIZE, OUTPUT_SIZE, 3), dtype=np.uint8)  # RGB image\n",
    "    \n",
    "    img_new_size = OUTPUT_SIZE // MAX_EMOJIS_PER_ROW\n",
    "\n",
    "    if randomise:\n",
    "      while len(units) < MAX_EMOJI_SEQUENCE_LENGTH:\n",
    "        units.append(' ')\n",
    "      np.random.shuffle(units)\n",
    "\n",
    "    generated = True\n",
    "    # Generate the image\n",
    "    for j in range(len(units)):\n",
    "      unit = units[j]\n",
    "      if unit == ' ':\n",
    "        continue\n",
    "      img = get_png_image_from_local_repo(unit, local_image_dir, filename_generator_fn)\n",
    "      if img is None:\n",
    "        undone += 1\n",
    "        generated = False\n",
    "        break\n",
    "\n",
    "      # Ensure img is in RGB before placing it on the canvas\n",
    "      if img.shape[2] == 4:  # If the image is RGBA, convert it to RGB\n",
    "          img = img[..., :3]\n",
    "\n",
    "      # Write the img to canvas starting at x, y\n",
    "      x_pos = j % MAX_EMOJIS_PER_ROW\n",
    "      y_pos = j // MAX_EMOJIS_PER_ROW\n",
    "      x = x_pos * img_new_size\n",
    "      y = y_pos * img_new_size\n",
    "      canvas[y:y+img.shape[0], x:x+img.shape[1]] = img\n",
    "    \n",
    "    if generated:\n",
    "      cv2.imwrite(output_filename, canvas)\n",
    "  \n",
    "  print(f\"Undone: {undone}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_to_noto_filename(emoji, extension=\"png\"):\n",
    "    # Step 1: Get the Unicode code points of the emoji.\n",
    "    codepoints = [f\"U{ord(char):04X}\" for char in emoji]\n",
    "    # Step 2: Join the code points with underscores for ZWJ (Zero-Width Joiner) support.\n",
    "    # Replace the \"U\" prefix with a lowercase \"u\" and remove the \"+\" symbol.\n",
    "    file_name_parts = [f\"u{codepoints[0][1:].lower()}\"]  # First code point with \"u\"\n",
    "    for codepoint in codepoints[1:]:\n",
    "        str = codepoint[1:].lower()\n",
    "        if str == 'fe0f': # Remove the variation selector\n",
    "          continue\n",
    "        file_name_parts.append(codepoint[1:].lower())  # Following code points without \"u\"\n",
    "    \n",
    "    # Step 3: Construct the filename (e.g., emoji_u1f9cf_200d_2640.png)\n",
    "    file_name = \"_\".join(file_name_parts)\n",
    "    return f\"emoji_{file_name}.{extension}\"\n",
    "\n",
    "output_folder = 'google_dataset'\n",
    "local_image_dir = 'noto-emoji-main/noto-emoji-main/png/72'\n",
    "\n",
    "generate_dataset(local_image_dir, output_folder, emoji_to_noto_filename, randomise=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_to_twemoji_filename(emoji, extension=\"png\"):\n",
    "    # Step 1: Get the Unicode code points of the emoji.\n",
    "    codepoints = [f\"U{ord(char):X}\" for char in emoji]\n",
    "    # Step 2: Join the code points with underscores for ZWJ (Zero-Width Joiner) support.\n",
    "    # Replace the \"U\" prefix with a lowercase \"u\" and remove the \"+\" symbol.\n",
    "    file_name_parts = []\n",
    "    for codepoint in codepoints:\n",
    "        str = codepoint[1:].lower()\n",
    "        if str == 'fe0f' and len(codepoints) <= 2: # Remove the variation selector if there's only 1 proper token\n",
    "          continue\n",
    "        file_name_parts.append(codepoint[1:].lower())  # Following code points without \"u\"\n",
    "    \n",
    "    # Step 3: Construct the filename (e.g., emoji_u1f9cf_200d_2640.png)\n",
    "    file_name = \"-\".join(file_name_parts)\n",
    "    return f\"{file_name}.{extension}\"\n",
    "\n",
    "output_folder = 'twitter_dataset'\n",
    "local_image_dir = 'twemoji-main/twemoji-main/assets/72x72'\n",
    "\n",
    "generate_dataset(local_image_dir, output_folder, emoji_to_twemoji_filename, randomise=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
