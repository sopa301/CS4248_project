{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "necktie [EM] chart_increasing.\n",
      "[':necktie:', ':chart_increasing:']\n"
     ]
    }
   ],
   "source": [
    "# Function to convert emojis to Unicode\n",
    "def emoji_to_unicode(emoji_str):\n",
    "    return ' '.join([f\"U+{ord(char):X}\" for char in emoji_str])\n",
    "\n",
    "def process_emoji_list_to_str(emoji_list):\n",
    "    desc_processed = ' [EM] '.join(desc.strip(':') for desc in emoji_list)\n",
    "    return f\"{desc_processed}.\"\n",
    "\n",
    "def unprocess_emoji_list_from_str(emoji_str):\n",
    "    s = emoji_str[:-1].split(' [EM] ')\n",
    "    return [f\":{desc}:\" for desc in s]\n",
    "\n",
    "import ast\n",
    "# write some tests for processing and unprocessing\n",
    "desc_list = ast.literal_eval(\"[':necktie:', ':chart_increasing:']\")\n",
    "print(process_emoji_list_to_str(desc_list))\n",
    "print(unprocess_emoji_list_from_str(process_emoji_list_to_str(desc_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete! Output saved to dataset_only_true.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import ast\n",
    "\n",
    "input_csv = 'ELCo.csv'\n",
    "output_csv = 'dataset_only_true.csv'\n",
    "\n",
    "\n",
    "with open(input_csv, newline='', encoding='utf-8') as fin, \\\n",
    "     open(output_csv, 'w', newline='', encoding='utf-8') as fout:\n",
    "\n",
    "    reader = csv.DictReader(fin)\n",
    "\n",
    "    # Update fieldnames to include all keys in writer.writerow()\n",
    "    fieldnames = ['sent1', 'sent2', 'unicode', 'label', 'strategy', 'attribute', 'filename', 'emoji']\n",
    "    writer = csv.DictWriter(fout, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    i = 0\n",
    "    for row in reader:\n",
    "        desc_list = ast.literal_eval(row['Description'])  # Convert string to list \n",
    "\n",
    "        sent1 = process_emoji_list_to_str(desc_list)\n",
    "        sent2 = row['EN']\n",
    "\n",
    "        label = 1\n",
    "\n",
    "        unicode_repr = emoji_to_unicode(row['EM'])\n",
    "\n",
    "        writer.writerow({\n",
    "            'sent1': sent1,\n",
    "            'sent2': sent2,\n",
    "            'unicode': unicode_repr,  # Added this to match fieldnames\n",
    "            'label': label,\n",
    "            'strategy': row['Composition strategy'],\n",
    "            'attribute': row['Attribute'],\n",
    "            'filename': f\"{i}.png\",\n",
    "            'emoji': row['EM'],\n",
    "        })\n",
    "\n",
    "        i += 1\n",
    "\n",
    "print(f\"Conversion complete! Output saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete! Output saved to dataset_only_false.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "input_csv_folder = 'original_ELCo_dataset'\n",
    "input_csvs = ['train.csv', 'val.csv', 'test.csv']\n",
    "output_csv = 'dataset_only_false.csv'\n",
    "reference_csv = 'ELCo_no_punctuation.csv'\n",
    "\n",
    "elco_df = pd.read_csv(reference_csv)\n",
    "\n",
    "with open(output_csv, 'w', newline='', encoding='utf-8') as fout:\n",
    "  \n",
    "    fieldnames = ['sent1', 'sent2', 'unicode', 'label', 'strategy', 'attribute', 'filename', 'emoji']\n",
    "    writer = csv.DictWriter(fout, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for input_csv in input_csvs:\n",
    "        with open(f\"{input_csv_folder}/{input_csv}\", newline='', encoding='utf-8') as fin:\n",
    "            reader = csv.DictReader(fin)\n",
    "\n",
    "            for row in reader:\n",
    "                if row['label'] == '1':\n",
    "                    continue\n",
    "                \n",
    "\n",
    "                sent1 = row['sent1']\n",
    "                # strip the 'this is ' prefix\n",
    "                tmp = sent1[8:]\n",
    "                # unprocess the emoji string\n",
    "                tmp = str(unprocess_emoji_list_from_str(tmp))\n",
    "\n",
    "                # get the unique row index of elco_df where the Description field matches tmp\n",
    "                elco_row = elco_df[elco_df['Description'] == tmp].index[0]\n",
    "\n",
    "                sent2 = row['sent2']\n",
    "                label = 0\n",
    "                unicode_repr = emoji_to_unicode(elco_df.loc[elco_row, 'EM'])\n",
    "\n",
    "                writer.writerow({\n",
    "                    'sent1': sent1,\n",
    "                    'sent2': sent2,\n",
    "                    'unicode': unicode_repr,\n",
    "                    'label': label,\n",
    "                    'strategy': elco_df.loc[elco_row, 'Composition strategy'],\n",
    "                    'attribute': elco_df.loc[elco_row, 'Attribute'],\n",
    "                    'filename': f\"{elco_row}.png\",\n",
    "                    'emoji': elco_df.loc[elco_row, 'EM'],\n",
    "                })\n",
    "\n",
    "print(f\"Conversion complete! Output saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Utility stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing numbers in filenames: [26, 42, 375, 376, 488, 544, 566, 622, 630, 638, 1343, 1521, 1523, 1528, 1529, 1530, 1531, 1533, 1534, 1537, 1539, 1541, 1545, 1551, 1553, 1554, 1555, 1556, 1558]\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def count_missing_numbers(folder_path):\n",
    "  # Get a list of all files in the folder\n",
    "  files = os.listdir(folder_path)\n",
    "  \n",
    "  # Extract numbers from filenames and convert them to integers\n",
    "  numbers = sorted([int(f.split('.')[0]) for f in files if f.split('.')[0].isdigit()])\n",
    "  \n",
    "  # Find the missing numbers\n",
    "  missing_numbers = [num for num in range(numbers[0], numbers[-1] + 1) if num not in numbers]\n",
    "  \n",
    "  return missing_numbers\n",
    "\n",
    "folder_path = 'google_dataset'\n",
    "missing_numbers = count_missing_numbers(folder_path)\n",
    "print(f\"Missing numbers in filenames: {missing_numbers}\")\n",
    "print(len(missing_numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of true rows: 1655\n",
      "Number of false rows: 1655\n",
      "Number of rows in train.csv: 2398\n",
      "Number of rows in val.csv: 394\n",
      "Number of rows in test.csv: 518\n",
      "Sums to: 3310\n"
     ]
    }
   ],
   "source": [
    "# count number of true and false rows in csvs\n",
    "import pandas as pd\n",
    "true_df = pd.read_csv('dataset_only_true.csv')\n",
    "false_df = pd.read_csv('dataset_only_false.csv')\n",
    "\n",
    "print(f\"Number of true rows: {len(true_df)}\")\n",
    "print(f\"Number of false rows: {len(false_df)}\")\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('original_ELCo_dataset/train.csv')\n",
    "val_df = pd.read_csv('original_ELCo_dataset/val.csv')\n",
    "test_df = pd.read_csv('original_ELCo_dataset/test.csv')\n",
    "\n",
    "print(f\"Number of rows in train.csv: {len(train_df)}\")\n",
    "print(f\"Number of rows in val.csv: {len(val_df)}\")\n",
    "print(f\"Number of rows in test.csv: {len(test_df)}\")\n",
    "print(f\"Sums to: {len(train_df) + len(val_df) + len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                               [👔, 📈]\n",
      "1                            [🏢, 🤑, 🤑]\n",
      "2                            [👨, 💻, 🤝]\n",
      "3       [🏢, 🧑, 🤝, 🧑, 🧑, 🤝, 🧑, 🧑, 🤝, 🧑]\n",
      "4                            [👩, 💻, 🤑]\n",
      "                     ...              \n",
      "1650                            [👍, 👣]\n",
      "1651                            [👏, 🪜]\n",
      "1652                         [😤, 🗣, 💬]\n",
      "1653                            [💨, 🤬]\n",
      "1654                         [👍, 👣, ➡]\n",
      "Name: emoji, Length: 3310, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import regex\n",
    "\n",
    "# Function to extract all emojis from a text string using Unicode properties.\n",
    "def extract_emojis(text):\n",
    "    emoji_pattern = regex.compile(r'\\p{Emoji}', flags=regex.UNICODE)\n",
    "    return emoji_pattern.findall(text) if isinstance(text, str) else []\n",
    "\n",
    "# Read the CSV files\n",
    "df_true = pd.read_csv('dataset_only_true.csv')\n",
    "df_false = pd.read_csv('dataset_only_false.csv')\n",
    "df = pd.concat([df_true, df_false])\n",
    "df_emoji = pd.read_csv('/home/andrew/CS4248_project/dataset/noto-emoji/emoji_dataset.csv')\n",
    "\n",
    "# Create a new column with the extracted emojis from the original 'emoji' column\n",
    "emojis = df['emoji'].apply(extract_emojis)\n",
    "print(emojis)\n",
    "file_name_list = []\n",
    "for emoji_list in emojis:\n",
    "    file_name = []\n",
    "    for emoji in emoji_list:\n",
    "        df_emoji_row = df_emoji[df_emoji['emoji'] == emoji]\n",
    "        if len(df_emoji_row) == 0:\n",
    "            print(f\"Emoji {emoji} not found in emoji dataset\")\n",
    "            continue\n",
    "        file_name.append(\"noto-emoji/png/512/\"+f\"{df_emoji_row['filename'].values[0]}\")\n",
    "    file_name_list.append(file_name)\n",
    "df['separate_filenames'] = file_name_list\n",
    "\n",
    "df.to_csv('merged_emoji.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (2648, 9)\n",
      "Validation shape: (331, 9)\n",
      "Test shape: (331, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "# Load the merged CSV file, preserving the header row.\n",
    "df = pd.read_csv(\"/home/andrew/CS4248_project/dataset/merged_emoji.csv\")  # header=0 by default\n",
    "\n",
    "# Split the data.\n",
    "# train: 80%, val: 10%, test: 10%\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save the splits to CSV files (the column names will be preserved).\n",
    "train_df.to_csv(\"/home/andrew/CS4248_project/dataset/train.csv\", index=False)\n",
    "val_df.to_csv(\"/home/andrew/CS4248_project/dataset/val.csv\", index=False)\n",
    "test_df.to_csv(\"/home/andrew/CS4248_project/dataset/test.csv\", index=False)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Validation shape:\", val_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                      [🎇, 🔮]\n",
      "1                      [🔆, 🔮]\n",
      "2                      [🌟, 🎓]\n",
      "3       [😎, 👨, 🔬, 👩, ⚕, 🧑, 💻]\n",
      "4                      [☀, 🔮]\n",
      "                ...          \n",
      "2393                [🔈, 🗑, 🚮]\n",
      "2394                   [👂, 📋]\n",
      "2395       [🤔, 🧠, 🔊, 🗣, ☺, 👍]\n",
      "2396          [🤔, 🧠, 🗣, 🙂, 🆗]\n",
      "2397                [🗣, 🧏, ♀]\n",
      "Name: EM, Length: 2398, dtype: object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                              [👔, 📈]\n",
      "1                           [🏢, 🤑, 🤑]\n",
      "2                           [👨, 💻, 🤝]\n",
      "3      [🏢, 🧑, 🤝, 🧑, 🧑, 🤝, 🧑, 🧑, 🤝, 🧑]\n",
      "4                           [👩, 💻, 🤑]\n",
      "                    ...              \n",
      "513                            [👍, 👣]\n",
      "514                            [👏, 🪜]\n",
      "515                         [😤, 🗣, 💬]\n",
      "516                            [💨, 🤬]\n",
      "517                         [👍, 👣, ➡]\n",
      "Name: EM, Length: 518, dtype: object\n",
      "0         [⏱, 🗺, 🥲]\n",
      "1            [👣, 🛣]\n",
      "2            [😪, 🗺]\n",
      "3      [⏳, 🚗, ✈, 🛤]\n",
      "4         [🛤, 🚢, 🗺]\n",
      "           ...     \n",
      "389       [🍃, 🥬, 🍃]\n",
      "390          [🆕, 👃]\n",
      "391    [🌬, 👃, 👌, 😌]\n",
      "392          [😋, 🥬]\n",
      "393       [🧼, 👃, 💨]\n",
      "Name: EM, Length: 394, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import regex\n",
    "\n",
    "# Function to extract all emojis from a text string using Unicode properties.\n",
    "def extract_emojis(text):\n",
    "    emoji_pattern = regex.compile(r'\\p{Emoji}', flags=regex.UNICODE)\n",
    "    return emoji_pattern.findall(text) if isinstance(text, str) else []\n",
    "\n",
    "# Read the CSV files\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_val = pd.read_csv('val.csv')\n",
    "df_emoji = pd.read_csv('/home/andrew/CS4248_project/dataset/noto-emoji/emoji_dataset.csv')\n",
    "\n",
    "# Create a new column with the extracted emojis from the original 'emoji' column\n",
    "for df in [df_train, df_test, df_val]:\n",
    "    emojis = df['EM'].apply(extract_emojis)\n",
    "    print(emojis)\n",
    "    file_name_list = []\n",
    "    for emoji_list in emojis:\n",
    "        file_name = []\n",
    "        for emoji in emoji_list:\n",
    "            df_emoji_row = df_emoji[df_emoji['emoji'] == emoji]\n",
    "            if len(df_emoji_row) == 0:\n",
    "                print(f\"Emoji {emoji} not found in emoji dataset\")\n",
    "                continue\n",
    "            file_name.append(\"noto-emoji/png/512/\"+f\"{df_emoji_row['filename'].values[0]}\")\n",
    "        file_name_list.append(file_name)\n",
    "    df['separate_filenames'] = file_name_list\n",
    "    \n",
    "    if df is df_train:\n",
    "        df.to_csv('train.csv', index=False)\n",
    "    elif df is df_test:\n",
    "        df.to_csv('test.csv', index=False)\n",
    "    elif df is df_val:\n",
    "        df.to_csv('val.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered                EM             EN  \\\n",
      "0              🎇🔮  bright future   \n",
      "1              🔆🔮  bright future   \n",
      "2              🌟🎓  bright future   \n",
      "3     😎👨‍🔬👩‍⚕️🧑‍💻  bright future   \n",
      "4             ☀️🔮  bright future   \n",
      "...           ...            ...   \n",
      "2393         🔈🗑️🚮   sound advice   \n",
      "2394           👂📋   sound advice   \n",
      "2395     🤔🧠🔊🗣️☺️👍   sound advice   \n",
      "2396       🤔🧠🗣️🙂🆗   sound advice   \n",
      "2397       🗣️🧏‍♀️   sound advice   \n",
      "\n",
      "                                                unicode  label  strategy  \\\n",
      "0                                       U+1F387 U+1F52E      1         0   \n",
      "1                                       U+1F506 U+1F52E      1         0   \n",
      "2                                       U+1F31F U+1F393      1         1   \n",
      "3     U+1F60E U+1F468 U+200D U+1F52C U+1F469 U+200D ...      1         2   \n",
      "4                                 U+2600 U+FE0F U+1F52E      1         0   \n",
      "...                                                 ...    ...       ...   \n",
      "2393                     U+1F508 U+1F5D1 U+FE0F U+1F6AE      0         6   \n",
      "2394                                    U+1F442 U+1F4CB      0         6   \n",
      "2395  U+1F914 U+1F9E0 U+1F50A U+1F5E3 U+FE0F U+263A ...      0         6   \n",
      "2396     U+1F914 U+1F9E0 U+1F5E3 U+FE0F U+1F642 U+1F197      0         6   \n",
      "2397        U+1F5E3 U+FE0F U+1F9CF U+200D U+2640 U+FE0F      0         6   \n",
      "\n",
      "                                              image  \\\n",
      "0        ./generated_img_dataset/train_google/0.png   \n",
      "1        ./generated_img_dataset/train_google/1.png   \n",
      "2        ./generated_img_dataset/train_google/2.png   \n",
      "3        ./generated_img_dataset/train_google/3.png   \n",
      "4        ./generated_img_dataset/train_google/4.png   \n",
      "...                                             ...   \n",
      "2393  ./generated_img_dataset/train_google/2393.png   \n",
      "2394  ./generated_img_dataset/train_google/2394.png   \n",
      "2395  ./generated_img_dataset/train_google/2395.png   \n",
      "2396  ./generated_img_dataset/train_google/2396.png   \n",
      "2397  ./generated_img_dataset/train_google/2397.png   \n",
      "\n",
      "                                     separate_filenames  \n",
      "0     ['noto-emoji/png/512/emoji_u1f387.png', 'noto-...  \n",
      "1     ['noto-emoji/png/512/emoji_u1f506.png', 'noto-...  \n",
      "2     ['noto-emoji/png/512/emoji_u1f31f.png', 'noto-...  \n",
      "3     ['noto-emoji/png/512/emoji_u1f60e.png', 'noto-...  \n",
      "4     ['noto-emoji/png/512/emoji_u2600.png', 'noto-e...  \n",
      "...                                                 ...  \n",
      "2393  ['noto-emoji/png/512/emoji_u1f508.png', 'noto-...  \n",
      "2394  ['noto-emoji/png/512/emoji_u1f442.png', 'noto-...  \n",
      "2395  ['noto-emoji/png/512/emoji_u1f914.png', 'noto-...  \n",
      "2396  ['noto-emoji/png/512/emoji_u1f914.png', 'noto-...  \n",
      "2397  ['noto-emoji/png/512/emoji_u1f5e3.png', 'noto-...  \n",
      "\n",
      "[2398 rows x 7 columns] saved as                EM             EN  \\\n",
      "0              🎇🔮  bright future   \n",
      "1              🔆🔮  bright future   \n",
      "2              🌟🎓  bright future   \n",
      "3     😎👨‍🔬👩‍⚕️🧑‍💻  bright future   \n",
      "4             ☀️🔮  bright future   \n",
      "...           ...            ...   \n",
      "2393         🔈🗑️🚮   sound advice   \n",
      "2394           👂📋   sound advice   \n",
      "2395     🤔🧠🔊🗣️☺️👍   sound advice   \n",
      "2396       🤔🧠🗣️🙂🆗   sound advice   \n",
      "2397       🗣️🧏‍♀️   sound advice   \n",
      "\n",
      "                                                unicode  label  strategy  \\\n",
      "0                                       U+1F387 U+1F52E      1         0   \n",
      "1                                       U+1F506 U+1F52E      1         0   \n",
      "2                                       U+1F31F U+1F393      1         1   \n",
      "3     U+1F60E U+1F468 U+200D U+1F52C U+1F469 U+200D ...      1         2   \n",
      "4                                 U+2600 U+FE0F U+1F52E      1         0   \n",
      "...                                                 ...    ...       ...   \n",
      "2393                     U+1F508 U+1F5D1 U+FE0F U+1F6AE      0         6   \n",
      "2394                                    U+1F442 U+1F4CB      0         6   \n",
      "2395  U+1F914 U+1F9E0 U+1F50A U+1F5E3 U+FE0F U+263A ...      0         6   \n",
      "2396     U+1F914 U+1F9E0 U+1F5E3 U+FE0F U+1F642 U+1F197      0         6   \n",
      "2397        U+1F5E3 U+FE0F U+1F9CF U+200D U+2640 U+FE0F      0         6   \n",
      "\n",
      "                                              image  \\\n",
      "0        ./generated_img_dataset/train_google/0.png   \n",
      "1        ./generated_img_dataset/train_google/1.png   \n",
      "2        ./generated_img_dataset/train_google/2.png   \n",
      "3        ./generated_img_dataset/train_google/3.png   \n",
      "4        ./generated_img_dataset/train_google/4.png   \n",
      "...                                             ...   \n",
      "2393  ./generated_img_dataset/train_google/2393.png   \n",
      "2394  ./generated_img_dataset/train_google/2394.png   \n",
      "2395  ./generated_img_dataset/train_google/2395.png   \n",
      "2396  ./generated_img_dataset/train_google/2396.png   \n",
      "2397  ./generated_img_dataset/train_google/2397.png   \n",
      "\n",
      "                                     separate_filenames  \n",
      "0     ['noto-emoji/png/512/emoji_u1f387.png', 'noto-...  \n",
      "1     ['noto-emoji/png/512/emoji_u1f506.png', 'noto-...  \n",
      "2     ['noto-emoji/png/512/emoji_u1f31f.png', 'noto-...  \n",
      "3     ['noto-emoji/png/512/emoji_u1f60e.png', 'noto-...  \n",
      "4     ['noto-emoji/png/512/emoji_u2600.png', 'noto-e...  \n",
      "...                                                 ...  \n",
      "2393  ['noto-emoji/png/512/emoji_u1f508.png', 'noto-...  \n",
      "2394  ['noto-emoji/png/512/emoji_u1f442.png', 'noto-...  \n",
      "2395  ['noto-emoji/png/512/emoji_u1f914.png', 'noto-...  \n",
      "2396  ['noto-emoji/png/512/emoji_u1f914.png', 'noto-...  \n",
      "2397  ['noto-emoji/png/512/emoji_u1f5e3.png', 'noto-...  \n",
      "\n",
      "[2398 rows x 7 columns]_only_emoji.csv\n",
      "Filtered                    EM                  EN  \\\n",
      "0                  👔📈        big business   \n",
      "1                 🏢🤑🤑        big business   \n",
      "2                👨‍💻🤝        big business   \n",
      "3    🏢🧑‍🤝‍🧑🧑‍🤝‍🧑🧑‍🤝‍🧑        big business   \n",
      "4                👩‍💻🤑        big business   \n",
      "..                ...                 ...   \n",
      "513                👍👣  effective entrance   \n",
      "514                👏🪜  effective entrance   \n",
      "515              😤🗣️💬  effective entrance   \n",
      "516                💨🤬  effective entrance   \n",
      "517              👍👣➡️  effective entrance   \n",
      "\n",
      "                                               unicode  label  strategy  \\\n",
      "0                                      U+1F454 U+1F4C8      1         1   \n",
      "1                              U+1F3E2 U+1F911 U+1F911      1         1   \n",
      "2                       U+1F468 U+200D U+1F4BB U+1F91D      1         1   \n",
      "3    U+1F3E2 U+1F9D1 U+200D U+1F91D U+200D U+1F9D1 ...      1         1   \n",
      "4                       U+1F469 U+200D U+1F4BB U+1F911      1         1   \n",
      "..                                                 ...    ...       ...   \n",
      "513                                    U+1F44D U+1F463      0         6   \n",
      "514                                    U+1F44F U+1FA9C      0         6   \n",
      "515                     U+1F624 U+1F5E3 U+FE0F U+1F4AC      0         6   \n",
      "516                                    U+1F4A8 U+1F92C      0         6   \n",
      "517                      U+1F44D U+1F463 U+27A1 U+FE0F      0         6   \n",
      "\n",
      "                                           image  \\\n",
      "0      ./generated_img_dataset/test_google/0.png   \n",
      "1      ./generated_img_dataset/test_google/1.png   \n",
      "2      ./generated_img_dataset/test_google/2.png   \n",
      "3      ./generated_img_dataset/test_google/3.png   \n",
      "4      ./generated_img_dataset/test_google/4.png   \n",
      "..                                           ...   \n",
      "513  ./generated_img_dataset/test_google/513.png   \n",
      "514  ./generated_img_dataset/test_google/514.png   \n",
      "515  ./generated_img_dataset/test_google/515.png   \n",
      "516  ./generated_img_dataset/test_google/516.png   \n",
      "517  ./generated_img_dataset/test_google/517.png   \n",
      "\n",
      "                                    separate_filenames  \n",
      "0    ['noto-emoji/png/512/emoji_u1f454.png', 'noto-...  \n",
      "1    ['noto-emoji/png/512/emoji_u1f3e2.png', 'noto-...  \n",
      "2    ['noto-emoji/png/512/emoji_u1f468.png', 'noto-...  \n",
      "3    ['noto-emoji/png/512/emoji_u1f3e2.png', 'noto-...  \n",
      "4    ['noto-emoji/png/512/emoji_u1f469.png', 'noto-...  \n",
      "..                                                 ...  \n",
      "513  ['noto-emoji/png/512/emoji_u1f44d.png', 'noto-...  \n",
      "514  ['noto-emoji/png/512/emoji_u1f44f.png', 'noto-...  \n",
      "515  ['noto-emoji/png/512/emoji_u1f624.png', 'noto-...  \n",
      "516  ['noto-emoji/png/512/emoji_u1f4a8.png', 'noto-...  \n",
      "517  ['noto-emoji/png/512/emoji_u1f44d.png', 'noto-...  \n",
      "\n",
      "[518 rows x 7 columns] saved as                    EM                  EN  \\\n",
      "0                  👔📈        big business   \n",
      "1                 🏢🤑🤑        big business   \n",
      "2                👨‍💻🤝        big business   \n",
      "3    🏢🧑‍🤝‍🧑🧑‍🤝‍🧑🧑‍🤝‍🧑        big business   \n",
      "4                👩‍💻🤑        big business   \n",
      "..                ...                 ...   \n",
      "513                👍👣  effective entrance   \n",
      "514                👏🪜  effective entrance   \n",
      "515              😤🗣️💬  effective entrance   \n",
      "516                💨🤬  effective entrance   \n",
      "517              👍👣➡️  effective entrance   \n",
      "\n",
      "                                               unicode  label  strategy  \\\n",
      "0                                      U+1F454 U+1F4C8      1         1   \n",
      "1                              U+1F3E2 U+1F911 U+1F911      1         1   \n",
      "2                       U+1F468 U+200D U+1F4BB U+1F91D      1         1   \n",
      "3    U+1F3E2 U+1F9D1 U+200D U+1F91D U+200D U+1F9D1 ...      1         1   \n",
      "4                       U+1F469 U+200D U+1F4BB U+1F911      1         1   \n",
      "..                                                 ...    ...       ...   \n",
      "513                                    U+1F44D U+1F463      0         6   \n",
      "514                                    U+1F44F U+1FA9C      0         6   \n",
      "515                     U+1F624 U+1F5E3 U+FE0F U+1F4AC      0         6   \n",
      "516                                    U+1F4A8 U+1F92C      0         6   \n",
      "517                      U+1F44D U+1F463 U+27A1 U+FE0F      0         6   \n",
      "\n",
      "                                           image  \\\n",
      "0      ./generated_img_dataset/test_google/0.png   \n",
      "1      ./generated_img_dataset/test_google/1.png   \n",
      "2      ./generated_img_dataset/test_google/2.png   \n",
      "3      ./generated_img_dataset/test_google/3.png   \n",
      "4      ./generated_img_dataset/test_google/4.png   \n",
      "..                                           ...   \n",
      "513  ./generated_img_dataset/test_google/513.png   \n",
      "514  ./generated_img_dataset/test_google/514.png   \n",
      "515  ./generated_img_dataset/test_google/515.png   \n",
      "516  ./generated_img_dataset/test_google/516.png   \n",
      "517  ./generated_img_dataset/test_google/517.png   \n",
      "\n",
      "                                    separate_filenames  \n",
      "0    ['noto-emoji/png/512/emoji_u1f454.png', 'noto-...  \n",
      "1    ['noto-emoji/png/512/emoji_u1f3e2.png', 'noto-...  \n",
      "2    ['noto-emoji/png/512/emoji_u1f468.png', 'noto-...  \n",
      "3    ['noto-emoji/png/512/emoji_u1f3e2.png', 'noto-...  \n",
      "4    ['noto-emoji/png/512/emoji_u1f469.png', 'noto-...  \n",
      "..                                                 ...  \n",
      "513  ['noto-emoji/png/512/emoji_u1f44d.png', 'noto-...  \n",
      "514  ['noto-emoji/png/512/emoji_u1f44f.png', 'noto-...  \n",
      "515  ['noto-emoji/png/512/emoji_u1f624.png', 'noto-...  \n",
      "516  ['noto-emoji/png/512/emoji_u1f4a8.png', 'noto-...  \n",
      "517  ['noto-emoji/png/512/emoji_u1f44d.png', 'noto-...  \n",
      "\n",
      "[518 rows x 7 columns]_only_emoji.csv\n",
      "Filtered          EM           EN                                      unicode  label  \\\n",
      "0     ⏱️🗺️🥲  far journey         U+23F1 U+FE0F U+1F5FA U+FE0F U+1F972      1   \n",
      "1       👣🛣️  far journey                       U+1F463 U+1F6E3 U+FE0F      1   \n",
      "2       😪🗺️  far journey                       U+1F62A U+1F5FA U+FE0F      1   \n",
      "3    ⏳🚗✈️🛤️  far journey  U+23F3 U+1F697 U+2708 U+FE0F U+1F6E4 U+FE0F      1   \n",
      "4     🛤️🚢🗺️  far journey        U+1F6E4 U+FE0F U+1F6A2 U+1F5FA U+FE0F      1   \n",
      "..      ...          ...                                          ...    ...   \n",
      "389     🍃🥬🍃    fresh air                      U+1F343 U+1F96C U+1F343      0   \n",
      "390      🆕👃    fresh air                              U+1F195 U+1F443      0   \n",
      "391   🌬️👃👌😌    fresh air       U+1F32C U+FE0F U+1F443 U+1F44C U+1F60C      0   \n",
      "392      😋🥬    fresh air                              U+1F60B U+1F96C      0   \n",
      "393     🧼👃💨    fresh air                      U+1F9FC U+1F443 U+1F4A8      0   \n",
      "\n",
      "     strategy                                       image  \\\n",
      "0           0    ./generated_img_dataset/val_google/0.png   \n",
      "1           1    ./generated_img_dataset/val_google/1.png   \n",
      "2           1    ./generated_img_dataset/val_google/2.png   \n",
      "3           0    ./generated_img_dataset/val_google/3.png   \n",
      "4           1    ./generated_img_dataset/val_google/4.png   \n",
      "..        ...                                         ...   \n",
      "389         6  ./generated_img_dataset/val_google/389.png   \n",
      "390         6  ./generated_img_dataset/val_google/390.png   \n",
      "391         6  ./generated_img_dataset/val_google/391.png   \n",
      "392         6  ./generated_img_dataset/val_google/392.png   \n",
      "393         6  ./generated_img_dataset/val_google/393.png   \n",
      "\n",
      "                                    separate_filenames  \n",
      "0    ['noto-emoji/png/512/emoji_u23f1.png', 'noto-e...  \n",
      "1    ['noto-emoji/png/512/emoji_u1f463.png', 'noto-...  \n",
      "2    ['noto-emoji/png/512/emoji_u1f62a.png', 'noto-...  \n",
      "3    ['noto-emoji/png/512/emoji_u23f3.png', 'noto-e...  \n",
      "4    ['noto-emoji/png/512/emoji_u1f6e4.png', 'noto-...  \n",
      "..                                                 ...  \n",
      "389  ['noto-emoji/png/512/emoji_u1f343.png', 'noto-...  \n",
      "390  ['noto-emoji/png/512/emoji_u1f195.png', 'noto-...  \n",
      "391  ['noto-emoji/png/512/emoji_u1f32c.png', 'noto-...  \n",
      "392  ['noto-emoji/png/512/emoji_u1f60b.png', 'noto-...  \n",
      "393  ['noto-emoji/png/512/emoji_u1f9fc.png', 'noto-...  \n",
      "\n",
      "[394 rows x 7 columns] saved as          EM           EN                                      unicode  label  \\\n",
      "0     ⏱️🗺️🥲  far journey         U+23F1 U+FE0F U+1F5FA U+FE0F U+1F972      1   \n",
      "1       👣🛣️  far journey                       U+1F463 U+1F6E3 U+FE0F      1   \n",
      "2       😪🗺️  far journey                       U+1F62A U+1F5FA U+FE0F      1   \n",
      "3    ⏳🚗✈️🛤️  far journey  U+23F3 U+1F697 U+2708 U+FE0F U+1F6E4 U+FE0F      1   \n",
      "4     🛤️🚢🗺️  far journey        U+1F6E4 U+FE0F U+1F6A2 U+1F5FA U+FE0F      1   \n",
      "..      ...          ...                                          ...    ...   \n",
      "389     🍃🥬🍃    fresh air                      U+1F343 U+1F96C U+1F343      0   \n",
      "390      🆕👃    fresh air                              U+1F195 U+1F443      0   \n",
      "391   🌬️👃👌😌    fresh air       U+1F32C U+FE0F U+1F443 U+1F44C U+1F60C      0   \n",
      "392      😋🥬    fresh air                              U+1F60B U+1F96C      0   \n",
      "393     🧼👃💨    fresh air                      U+1F9FC U+1F443 U+1F4A8      0   \n",
      "\n",
      "     strategy                                       image  \\\n",
      "0           0    ./generated_img_dataset/val_google/0.png   \n",
      "1           1    ./generated_img_dataset/val_google/1.png   \n",
      "2           1    ./generated_img_dataset/val_google/2.png   \n",
      "3           0    ./generated_img_dataset/val_google/3.png   \n",
      "4           1    ./generated_img_dataset/val_google/4.png   \n",
      "..        ...                                         ...   \n",
      "389         6  ./generated_img_dataset/val_google/389.png   \n",
      "390         6  ./generated_img_dataset/val_google/390.png   \n",
      "391         6  ./generated_img_dataset/val_google/391.png   \n",
      "392         6  ./generated_img_dataset/val_google/392.png   \n",
      "393         6  ./generated_img_dataset/val_google/393.png   \n",
      "\n",
      "                                    separate_filenames  \n",
      "0    ['noto-emoji/png/512/emoji_u23f1.png', 'noto-e...  \n",
      "1    ['noto-emoji/png/512/emoji_u1f463.png', 'noto-...  \n",
      "2    ['noto-emoji/png/512/emoji_u1f62a.png', 'noto-...  \n",
      "3    ['noto-emoji/png/512/emoji_u23f3.png', 'noto-e...  \n",
      "4    ['noto-emoji/png/512/emoji_u1f6e4.png', 'noto-...  \n",
      "..                                                 ...  \n",
      "389  ['noto-emoji/png/512/emoji_u1f343.png', 'noto-...  \n",
      "390  ['noto-emoji/png/512/emoji_u1f195.png', 'noto-...  \n",
      "391  ['noto-emoji/png/512/emoji_u1f32c.png', 'noto-...  \n",
      "392  ['noto-emoji/png/512/emoji_u1f60b.png', 'noto-...  \n",
      "393  ['noto-emoji/png/512/emoji_u1f9fc.png', 'noto-...  \n",
      "\n",
      "[394 rows x 7 columns]_only_emoji.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the CSV\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_val = pd.read_csv(\"val.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "# Function to check if string contains ONLY non-English characters\n",
    "def is_pure_emoji(text):\n",
    "    return not bool(re.search(r'[A-Za-z]', str(text)))\n",
    "for df in [df_train, df_test, df_val]:\n",
    "\n",
    "\n",
    "    # Keep only rows where EM is pure emoji (no English letters)\n",
    "    df_filtered = df[df['EM'].apply(is_pure_emoji)]\n",
    "\n",
    "    # Save the result\n",
    "    if df is df_train:\n",
    "        df_filtered.to_csv(\"train_only_emoji.csv\", index=False)\n",
    "    elif df is df_val:\n",
    "        df_filtered.to_csv(\"val_only_emoji.csv\", index=False)\n",
    "    elif df is df_test:\n",
    "        df_filtered.to_csv(\"test_only_emoji.csv\", index=False)\n",
    "    print(f\"Filtered {df} saved as {df}_only_emoji.csv\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPA2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
