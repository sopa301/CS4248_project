{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "necktie [EM] chart_increasing.\n",
      "[':necktie:', ':chart_increasing:']\n"
     ]
    }
   ],
   "source": [
    "# Function to convert emojis to Unicode\n",
    "def emoji_to_unicode(emoji_str):\n",
    "    return ' '.join([f\"U+{ord(char):X}\" for char in emoji_str])\n",
    "\n",
    "def process_emoji_list_to_str(emoji_list):\n",
    "    desc_processed = ' [EM] '.join(desc.strip(':') for desc in emoji_list)\n",
    "    return f\"{desc_processed}.\"\n",
    "\n",
    "def unprocess_emoji_list_from_str(emoji_str):\n",
    "    s = emoji_str[:-1].split(' [EM] ')\n",
    "    return [f\":{desc}:\" for desc in s]\n",
    "\n",
    "import ast\n",
    "# write some tests for processing and unprocessing\n",
    "desc_list = ast.literal_eval(\"[':necktie:', ':chart_increasing:']\")\n",
    "print(process_emoji_list_to_str(desc_list))\n",
    "print(unprocess_emoji_list_from_str(process_emoji_list_to_str(desc_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete! Output saved to dataset_only_true.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import ast\n",
    "\n",
    "input_csv = 'ELCo.csv'\n",
    "output_csv = 'dataset_only_true.csv'\n",
    "\n",
    "\n",
    "with open(input_csv, newline='', encoding='utf-8') as fin, \\\n",
    "     open(output_csv, 'w', newline='', encoding='utf-8') as fout:\n",
    "\n",
    "    reader = csv.DictReader(fin)\n",
    "\n",
    "    # Update fieldnames to include all keys in writer.writerow()\n",
    "    fieldnames = ['sent1', 'sent2', 'unicode', 'label', 'strategy', 'attribute', 'filename', 'emoji']\n",
    "    writer = csv.DictWriter(fout, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    i = 0\n",
    "    for row in reader:\n",
    "        desc_list = ast.literal_eval(row['Description'])  # Convert string to list \n",
    "\n",
    "        sent1 = process_emoji_list_to_str(desc_list)\n",
    "        sent2 = row['EN']\n",
    "\n",
    "        label = 1\n",
    "\n",
    "        unicode_repr = emoji_to_unicode(row['EM'])\n",
    "\n",
    "        writer.writerow({\n",
    "            'sent1': sent1,\n",
    "            'sent2': sent2,\n",
    "            'unicode': unicode_repr,  # Added this to match fieldnames\n",
    "            'label': label,\n",
    "            'strategy': row['Composition strategy'],\n",
    "            'attribute': row['Attribute'],\n",
    "            'filename': f\"{i}.png\",\n",
    "            'emoji': row['EM'],\n",
    "        })\n",
    "\n",
    "        i += 1\n",
    "\n",
    "print(f\"Conversion complete! Output saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete! Output saved to dataset_only_false.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "input_csv_folder = 'original_ELCo_dataset'\n",
    "input_csvs = ['train.csv', 'val.csv', 'test.csv']\n",
    "output_csv = 'dataset_only_false.csv'\n",
    "reference_csv = 'ELCo_no_punctuation.csv'\n",
    "\n",
    "elco_df = pd.read_csv(reference_csv)\n",
    "\n",
    "with open(output_csv, 'w', newline='', encoding='utf-8') as fout:\n",
    "  \n",
    "    fieldnames = ['sent1', 'sent2', 'unicode', 'label', 'strategy', 'attribute', 'filename', 'emoji']\n",
    "    writer = csv.DictWriter(fout, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for input_csv in input_csvs:\n",
    "        with open(f\"{input_csv_folder}/{input_csv}\", newline='', encoding='utf-8') as fin:\n",
    "            reader = csv.DictReader(fin)\n",
    "\n",
    "            for row in reader:\n",
    "                if row['label'] == '1':\n",
    "                    continue\n",
    "                \n",
    "\n",
    "                sent1 = row['sent1']\n",
    "                # strip the 'this is ' prefix\n",
    "                tmp = sent1[8:]\n",
    "                # unprocess the emoji string\n",
    "                tmp = str(unprocess_emoji_list_from_str(tmp))\n",
    "\n",
    "                # get the unique row index of elco_df where the Description field matches tmp\n",
    "                elco_row = elco_df[elco_df['Description'] == tmp].index[0]\n",
    "\n",
    "                sent2 = row['sent2']\n",
    "                label = 0\n",
    "                unicode_repr = emoji_to_unicode(elco_df.loc[elco_row, 'EM'])\n",
    "\n",
    "                writer.writerow({\n",
    "                    'sent1': sent1,\n",
    "                    'sent2': sent2,\n",
    "                    'unicode': unicode_repr,\n",
    "                    'label': label,\n",
    "                    'strategy': elco_df.loc[elco_row, 'Composition strategy'],\n",
    "                    'attribute': elco_df.loc[elco_row, 'Attribute'],\n",
    "                    'filename': f\"{elco_row}.png\",\n",
    "                    'emoji': elco_df.loc[elco_row, 'EM'],\n",
    "                })\n",
    "\n",
    "print(f\"Conversion complete! Output saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Utility stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing numbers in filenames: [26, 42, 375, 376, 488, 544, 566, 622, 630, 638, 1343, 1521, 1523, 1528, 1529, 1530, 1531, 1533, 1534, 1537, 1539, 1541, 1545, 1551, 1553, 1554, 1555, 1556, 1558]\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def count_missing_numbers(folder_path):\n",
    "  # Get a list of all files in the folder\n",
    "  files = os.listdir(folder_path)\n",
    "  \n",
    "  # Extract numbers from filenames and convert them to integers\n",
    "  numbers = sorted([int(f.split('.')[0]) for f in files if f.split('.')[0].isdigit()])\n",
    "  \n",
    "  # Find the missing numbers\n",
    "  missing_numbers = [num for num in range(numbers[0], numbers[-1] + 1) if num not in numbers]\n",
    "  \n",
    "  return missing_numbers\n",
    "\n",
    "folder_path = 'google_dataset'\n",
    "missing_numbers = count_missing_numbers(folder_path)\n",
    "print(f\"Missing numbers in filenames: {missing_numbers}\")\n",
    "print(len(missing_numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of true rows: 1655\n",
      "Number of false rows: 1655\n",
      "Number of rows in train.csv: 2398\n",
      "Number of rows in val.csv: 394\n",
      "Number of rows in test.csv: 518\n",
      "Sums to: 3310\n"
     ]
    }
   ],
   "source": [
    "# count number of true and false rows in csvs\n",
    "import pandas as pd\n",
    "true_df = pd.read_csv('dataset_only_true.csv')\n",
    "false_df = pd.read_csv('dataset_only_false.csv')\n",
    "\n",
    "print(f\"Number of true rows: {len(true_df)}\")\n",
    "print(f\"Number of false rows: {len(false_df)}\")\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('original_ELCo_dataset/train.csv')\n",
    "val_df = pd.read_csv('original_ELCo_dataset/val.csv')\n",
    "test_df = pd.read_csv('original_ELCo_dataset/test.csv')\n",
    "\n",
    "print(f\"Number of rows in train.csv: {len(train_df)}\")\n",
    "print(f\"Number of rows in val.csv: {len(val_df)}\")\n",
    "print(f\"Number of rows in test.csv: {len(test_df)}\")\n",
    "print(f\"Sums to: {len(train_df) + len(val_df) + len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                               [ğŸ‘”, ğŸ“ˆ]\n",
      "1                            [ğŸ¢, ğŸ¤‘, ğŸ¤‘]\n",
      "2                            [ğŸ‘¨, ğŸ’», ğŸ¤]\n",
      "3       [ğŸ¢, ğŸ§‘, ğŸ¤, ğŸ§‘, ğŸ§‘, ğŸ¤, ğŸ§‘, ğŸ§‘, ğŸ¤, ğŸ§‘]\n",
      "4                            [ğŸ‘©, ğŸ’», ğŸ¤‘]\n",
      "                     ...              \n",
      "1650                            [ğŸ‘, ğŸ‘£]\n",
      "1651                            [ğŸ‘, ğŸªœ]\n",
      "1652                         [ğŸ˜¤, ğŸ—£, ğŸ’¬]\n",
      "1653                            [ğŸ’¨, ğŸ¤¬]\n",
      "1654                         [ğŸ‘, ğŸ‘£, â¡]\n",
      "Name: emoji, Length: 3310, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import regex\n",
    "\n",
    "# Function to extract all emojis from a text string using Unicode properties.\n",
    "def extract_emojis(text):\n",
    "    emoji_pattern = regex.compile(r'\\p{Emoji}', flags=regex.UNICODE)\n",
    "    return emoji_pattern.findall(text) if isinstance(text, str) else []\n",
    "\n",
    "# Read the CSV files\n",
    "df_true = pd.read_csv('dataset_only_true.csv')\n",
    "df_false = pd.read_csv('dataset_only_false.csv')\n",
    "df = pd.concat([df_true, df_false])\n",
    "df_emoji = pd.read_csv('/home/andrew/CS4248_project/dataset/noto-emoji/emoji_dataset.csv')\n",
    "\n",
    "# Create a new column with the extracted emojis from the original 'emoji' column\n",
    "emojis = df['emoji'].apply(extract_emojis)\n",
    "print(emojis)\n",
    "file_name_list = []\n",
    "for emoji_list in emojis:\n",
    "    file_name = []\n",
    "    for emoji in emoji_list:\n",
    "        df_emoji_row = df_emoji[df_emoji['emoji'] == emoji]\n",
    "        if len(df_emoji_row) == 0:\n",
    "            print(f\"Emoji {emoji} not found in emoji dataset\")\n",
    "            continue\n",
    "        file_name.append(\"noto-emoji/png/512/\"+f\"{df_emoji_row['filename'].values[0]}\")\n",
    "    file_name_list.append(file_name)\n",
    "df['separate_filenames'] = file_name_list\n",
    "\n",
    "df.to_csv('merged_emoji.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (2648, 9)\n",
      "Validation shape: (331, 9)\n",
      "Test shape: (331, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "# Load the merged CSV file, preserving the header row.\n",
    "df = pd.read_csv(\"/home/andrew/CS4248_project/dataset/merged_emoji.csv\")  # header=0 by default\n",
    "\n",
    "# Split the data.\n",
    "# train: 80%, val: 10%, test: 10%\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save the splits to CSV files (the column names will be preserved).\n",
    "train_df.to_csv(\"/home/andrew/CS4248_project/dataset/train.csv\", index=False)\n",
    "val_df.to_csv(\"/home/andrew/CS4248_project/dataset/val.csv\", index=False)\n",
    "test_df.to_csv(\"/home/andrew/CS4248_project/dataset/test.csv\", index=False)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Validation shape:\", val_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                      [ğŸ‡, ğŸ”®]\n",
      "1                      [ğŸ”†, ğŸ”®]\n",
      "2                      [ğŸŒŸ, ğŸ“]\n",
      "3       [ğŸ˜, ğŸ‘¨, ğŸ”¬, ğŸ‘©, âš•, ğŸ§‘, ğŸ’»]\n",
      "4                      [â˜€, ğŸ”®]\n",
      "                ...          \n",
      "2393                [ğŸ”ˆ, ğŸ—‘, ğŸš®]\n",
      "2394                   [ğŸ‘‚, ğŸ“‹]\n",
      "2395       [ğŸ¤”, ğŸ§ , ğŸ”Š, ğŸ—£, â˜º, ğŸ‘]\n",
      "2396          [ğŸ¤”, ğŸ§ , ğŸ—£, ğŸ™‚, ğŸ†—]\n",
      "2397                [ğŸ—£, ğŸ§, â™€]\n",
      "Name: EM, Length: 2398, dtype: object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                              [ğŸ‘”, ğŸ“ˆ]\n",
      "1                           [ğŸ¢, ğŸ¤‘, ğŸ¤‘]\n",
      "2                           [ğŸ‘¨, ğŸ’», ğŸ¤]\n",
      "3      [ğŸ¢, ğŸ§‘, ğŸ¤, ğŸ§‘, ğŸ§‘, ğŸ¤, ğŸ§‘, ğŸ§‘, ğŸ¤, ğŸ§‘]\n",
      "4                           [ğŸ‘©, ğŸ’», ğŸ¤‘]\n",
      "                    ...              \n",
      "513                            [ğŸ‘, ğŸ‘£]\n",
      "514                            [ğŸ‘, ğŸªœ]\n",
      "515                         [ğŸ˜¤, ğŸ—£, ğŸ’¬]\n",
      "516                            [ğŸ’¨, ğŸ¤¬]\n",
      "517                         [ğŸ‘, ğŸ‘£, â¡]\n",
      "Name: EM, Length: 518, dtype: object\n",
      "0         [â±, ğŸ—º, ğŸ¥²]\n",
      "1            [ğŸ‘£, ğŸ›£]\n",
      "2            [ğŸ˜ª, ğŸ—º]\n",
      "3      [â³, ğŸš—, âœˆ, ğŸ›¤]\n",
      "4         [ğŸ›¤, ğŸš¢, ğŸ—º]\n",
      "           ...     \n",
      "389       [ğŸƒ, ğŸ¥¬, ğŸƒ]\n",
      "390          [ğŸ†•, ğŸ‘ƒ]\n",
      "391    [ğŸŒ¬, ğŸ‘ƒ, ğŸ‘Œ, ğŸ˜Œ]\n",
      "392          [ğŸ˜‹, ğŸ¥¬]\n",
      "393       [ğŸ§¼, ğŸ‘ƒ, ğŸ’¨]\n",
      "Name: EM, Length: 394, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import regex\n",
    "\n",
    "# Function to extract all emojis from a text string using Unicode properties.\n",
    "def extract_emojis(text):\n",
    "    emoji_pattern = regex.compile(r'\\p{Emoji}', flags=regex.UNICODE)\n",
    "    return emoji_pattern.findall(text) if isinstance(text, str) else []\n",
    "\n",
    "# Read the CSV files\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_val = pd.read_csv('val.csv')\n",
    "df_emoji = pd.read_csv('/home/andrew/CS4248_project/dataset/noto-emoji/emoji_dataset.csv')\n",
    "\n",
    "# Create a new column with the extracted emojis from the original 'emoji' column\n",
    "for df in [df_train, df_test, df_val]:\n",
    "    emojis = df['EM'].apply(extract_emojis)\n",
    "    print(emojis)\n",
    "    file_name_list = []\n",
    "    for emoji_list in emojis:\n",
    "        file_name = []\n",
    "        for emoji in emoji_list:\n",
    "            df_emoji_row = df_emoji[df_emoji['emoji'] == emoji]\n",
    "            if len(df_emoji_row) == 0:\n",
    "                print(f\"Emoji {emoji} not found in emoji dataset\")\n",
    "                continue\n",
    "            file_name.append(\"noto-emoji/png/512/\"+f\"{df_emoji_row['filename'].values[0]}\")\n",
    "        file_name_list.append(file_name)\n",
    "    df['separate_filenames'] = file_name_list\n",
    "    \n",
    "    if df is df_train:\n",
    "        df.to_csv('train.csv', index=False)\n",
    "    elif df is df_test:\n",
    "        df.to_csv('test.csv', index=False)\n",
    "    elif df is df_val:\n",
    "        df.to_csv('val.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered                EM             EN  \\\n",
      "0              ğŸ‡ğŸ”®  bright future   \n",
      "1              ğŸ”†ğŸ”®  bright future   \n",
      "2              ğŸŒŸğŸ“  bright future   \n",
      "3     ğŸ˜ğŸ‘¨â€ğŸ”¬ğŸ‘©â€âš•ï¸ğŸ§‘â€ğŸ’»  bright future   \n",
      "4             â˜€ï¸ğŸ”®  bright future   \n",
      "...           ...            ...   \n",
      "2393         ğŸ”ˆğŸ—‘ï¸ğŸš®   sound advice   \n",
      "2394           ğŸ‘‚ğŸ“‹   sound advice   \n",
      "2395     ğŸ¤”ğŸ§ ğŸ”ŠğŸ—£ï¸â˜ºï¸ğŸ‘   sound advice   \n",
      "2396       ğŸ¤”ğŸ§ ğŸ—£ï¸ğŸ™‚ğŸ†—   sound advice   \n",
      "2397       ğŸ—£ï¸ğŸ§â€â™€ï¸   sound advice   \n",
      "\n",
      "                                                unicode  label  strategy  \\\n",
      "0                                       U+1F387 U+1F52E      1         0   \n",
      "1                                       U+1F506 U+1F52E      1         0   \n",
      "2                                       U+1F31F U+1F393      1         1   \n",
      "3     U+1F60E U+1F468 U+200D U+1F52C U+1F469 U+200D ...      1         2   \n",
      "4                                 U+2600 U+FE0F U+1F52E      1         0   \n",
      "...                                                 ...    ...       ...   \n",
      "2393                     U+1F508 U+1F5D1 U+FE0F U+1F6AE      0         6   \n",
      "2394                                    U+1F442 U+1F4CB      0         6   \n",
      "2395  U+1F914 U+1F9E0 U+1F50A U+1F5E3 U+FE0F U+263A ...      0         6   \n",
      "2396     U+1F914 U+1F9E0 U+1F5E3 U+FE0F U+1F642 U+1F197      0         6   \n",
      "2397        U+1F5E3 U+FE0F U+1F9CF U+200D U+2640 U+FE0F      0         6   \n",
      "\n",
      "                                              image  \\\n",
      "0        ./generated_img_dataset/train_google/0.png   \n",
      "1        ./generated_img_dataset/train_google/1.png   \n",
      "2        ./generated_img_dataset/train_google/2.png   \n",
      "3        ./generated_img_dataset/train_google/3.png   \n",
      "4        ./generated_img_dataset/train_google/4.png   \n",
      "...                                             ...   \n",
      "2393  ./generated_img_dataset/train_google/2393.png   \n",
      "2394  ./generated_img_dataset/train_google/2394.png   \n",
      "2395  ./generated_img_dataset/train_google/2395.png   \n",
      "2396  ./generated_img_dataset/train_google/2396.png   \n",
      "2397  ./generated_img_dataset/train_google/2397.png   \n",
      "\n",
      "                                     separate_filenames  \n",
      "0     ['noto-emoji/png/512/emoji_u1f387.png', 'noto-...  \n",
      "1     ['noto-emoji/png/512/emoji_u1f506.png', 'noto-...  \n",
      "2     ['noto-emoji/png/512/emoji_u1f31f.png', 'noto-...  \n",
      "3     ['noto-emoji/png/512/emoji_u1f60e.png', 'noto-...  \n",
      "4     ['noto-emoji/png/512/emoji_u2600.png', 'noto-e...  \n",
      "...                                                 ...  \n",
      "2393  ['noto-emoji/png/512/emoji_u1f508.png', 'noto-...  \n",
      "2394  ['noto-emoji/png/512/emoji_u1f442.png', 'noto-...  \n",
      "2395  ['noto-emoji/png/512/emoji_u1f914.png', 'noto-...  \n",
      "2396  ['noto-emoji/png/512/emoji_u1f914.png', 'noto-...  \n",
      "2397  ['noto-emoji/png/512/emoji_u1f5e3.png', 'noto-...  \n",
      "\n",
      "[2398 rows x 7 columns] saved as                EM             EN  \\\n",
      "0              ğŸ‡ğŸ”®  bright future   \n",
      "1              ğŸ”†ğŸ”®  bright future   \n",
      "2              ğŸŒŸğŸ“  bright future   \n",
      "3     ğŸ˜ğŸ‘¨â€ğŸ”¬ğŸ‘©â€âš•ï¸ğŸ§‘â€ğŸ’»  bright future   \n",
      "4             â˜€ï¸ğŸ”®  bright future   \n",
      "...           ...            ...   \n",
      "2393         ğŸ”ˆğŸ—‘ï¸ğŸš®   sound advice   \n",
      "2394           ğŸ‘‚ğŸ“‹   sound advice   \n",
      "2395     ğŸ¤”ğŸ§ ğŸ”ŠğŸ—£ï¸â˜ºï¸ğŸ‘   sound advice   \n",
      "2396       ğŸ¤”ğŸ§ ğŸ—£ï¸ğŸ™‚ğŸ†—   sound advice   \n",
      "2397       ğŸ—£ï¸ğŸ§â€â™€ï¸   sound advice   \n",
      "\n",
      "                                                unicode  label  strategy  \\\n",
      "0                                       U+1F387 U+1F52E      1         0   \n",
      "1                                       U+1F506 U+1F52E      1         0   \n",
      "2                                       U+1F31F U+1F393      1         1   \n",
      "3     U+1F60E U+1F468 U+200D U+1F52C U+1F469 U+200D ...      1         2   \n",
      "4                                 U+2600 U+FE0F U+1F52E      1         0   \n",
      "...                                                 ...    ...       ...   \n",
      "2393                     U+1F508 U+1F5D1 U+FE0F U+1F6AE      0         6   \n",
      "2394                                    U+1F442 U+1F4CB      0         6   \n",
      "2395  U+1F914 U+1F9E0 U+1F50A U+1F5E3 U+FE0F U+263A ...      0         6   \n",
      "2396     U+1F914 U+1F9E0 U+1F5E3 U+FE0F U+1F642 U+1F197      0         6   \n",
      "2397        U+1F5E3 U+FE0F U+1F9CF U+200D U+2640 U+FE0F      0         6   \n",
      "\n",
      "                                              image  \\\n",
      "0        ./generated_img_dataset/train_google/0.png   \n",
      "1        ./generated_img_dataset/train_google/1.png   \n",
      "2        ./generated_img_dataset/train_google/2.png   \n",
      "3        ./generated_img_dataset/train_google/3.png   \n",
      "4        ./generated_img_dataset/train_google/4.png   \n",
      "...                                             ...   \n",
      "2393  ./generated_img_dataset/train_google/2393.png   \n",
      "2394  ./generated_img_dataset/train_google/2394.png   \n",
      "2395  ./generated_img_dataset/train_google/2395.png   \n",
      "2396  ./generated_img_dataset/train_google/2396.png   \n",
      "2397  ./generated_img_dataset/train_google/2397.png   \n",
      "\n",
      "                                     separate_filenames  \n",
      "0     ['noto-emoji/png/512/emoji_u1f387.png', 'noto-...  \n",
      "1     ['noto-emoji/png/512/emoji_u1f506.png', 'noto-...  \n",
      "2     ['noto-emoji/png/512/emoji_u1f31f.png', 'noto-...  \n",
      "3     ['noto-emoji/png/512/emoji_u1f60e.png', 'noto-...  \n",
      "4     ['noto-emoji/png/512/emoji_u2600.png', 'noto-e...  \n",
      "...                                                 ...  \n",
      "2393  ['noto-emoji/png/512/emoji_u1f508.png', 'noto-...  \n",
      "2394  ['noto-emoji/png/512/emoji_u1f442.png', 'noto-...  \n",
      "2395  ['noto-emoji/png/512/emoji_u1f914.png', 'noto-...  \n",
      "2396  ['noto-emoji/png/512/emoji_u1f914.png', 'noto-...  \n",
      "2397  ['noto-emoji/png/512/emoji_u1f5e3.png', 'noto-...  \n",
      "\n",
      "[2398 rows x 7 columns]_only_emoji.csv\n",
      "Filtered                    EM                  EN  \\\n",
      "0                  ğŸ‘”ğŸ“ˆ        big business   \n",
      "1                 ğŸ¢ğŸ¤‘ğŸ¤‘        big business   \n",
      "2                ğŸ‘¨â€ğŸ’»ğŸ¤        big business   \n",
      "3    ğŸ¢ğŸ§‘â€ğŸ¤â€ğŸ§‘ğŸ§‘â€ğŸ¤â€ğŸ§‘ğŸ§‘â€ğŸ¤â€ğŸ§‘        big business   \n",
      "4                ğŸ‘©â€ğŸ’»ğŸ¤‘        big business   \n",
      "..                ...                 ...   \n",
      "513                ğŸ‘ğŸ‘£  effective entrance   \n",
      "514                ğŸ‘ğŸªœ  effective entrance   \n",
      "515              ğŸ˜¤ğŸ—£ï¸ğŸ’¬  effective entrance   \n",
      "516                ğŸ’¨ğŸ¤¬  effective entrance   \n",
      "517              ğŸ‘ğŸ‘£â¡ï¸  effective entrance   \n",
      "\n",
      "                                               unicode  label  strategy  \\\n",
      "0                                      U+1F454 U+1F4C8      1         1   \n",
      "1                              U+1F3E2 U+1F911 U+1F911      1         1   \n",
      "2                       U+1F468 U+200D U+1F4BB U+1F91D      1         1   \n",
      "3    U+1F3E2 U+1F9D1 U+200D U+1F91D U+200D U+1F9D1 ...      1         1   \n",
      "4                       U+1F469 U+200D U+1F4BB U+1F911      1         1   \n",
      "..                                                 ...    ...       ...   \n",
      "513                                    U+1F44D U+1F463      0         6   \n",
      "514                                    U+1F44F U+1FA9C      0         6   \n",
      "515                     U+1F624 U+1F5E3 U+FE0F U+1F4AC      0         6   \n",
      "516                                    U+1F4A8 U+1F92C      0         6   \n",
      "517                      U+1F44D U+1F463 U+27A1 U+FE0F      0         6   \n",
      "\n",
      "                                           image  \\\n",
      "0      ./generated_img_dataset/test_google/0.png   \n",
      "1      ./generated_img_dataset/test_google/1.png   \n",
      "2      ./generated_img_dataset/test_google/2.png   \n",
      "3      ./generated_img_dataset/test_google/3.png   \n",
      "4      ./generated_img_dataset/test_google/4.png   \n",
      "..                                           ...   \n",
      "513  ./generated_img_dataset/test_google/513.png   \n",
      "514  ./generated_img_dataset/test_google/514.png   \n",
      "515  ./generated_img_dataset/test_google/515.png   \n",
      "516  ./generated_img_dataset/test_google/516.png   \n",
      "517  ./generated_img_dataset/test_google/517.png   \n",
      "\n",
      "                                    separate_filenames  \n",
      "0    ['noto-emoji/png/512/emoji_u1f454.png', 'noto-...  \n",
      "1    ['noto-emoji/png/512/emoji_u1f3e2.png', 'noto-...  \n",
      "2    ['noto-emoji/png/512/emoji_u1f468.png', 'noto-...  \n",
      "3    ['noto-emoji/png/512/emoji_u1f3e2.png', 'noto-...  \n",
      "4    ['noto-emoji/png/512/emoji_u1f469.png', 'noto-...  \n",
      "..                                                 ...  \n",
      "513  ['noto-emoji/png/512/emoji_u1f44d.png', 'noto-...  \n",
      "514  ['noto-emoji/png/512/emoji_u1f44f.png', 'noto-...  \n",
      "515  ['noto-emoji/png/512/emoji_u1f624.png', 'noto-...  \n",
      "516  ['noto-emoji/png/512/emoji_u1f4a8.png', 'noto-...  \n",
      "517  ['noto-emoji/png/512/emoji_u1f44d.png', 'noto-...  \n",
      "\n",
      "[518 rows x 7 columns] saved as                    EM                  EN  \\\n",
      "0                  ğŸ‘”ğŸ“ˆ        big business   \n",
      "1                 ğŸ¢ğŸ¤‘ğŸ¤‘        big business   \n",
      "2                ğŸ‘¨â€ğŸ’»ğŸ¤        big business   \n",
      "3    ğŸ¢ğŸ§‘â€ğŸ¤â€ğŸ§‘ğŸ§‘â€ğŸ¤â€ğŸ§‘ğŸ§‘â€ğŸ¤â€ğŸ§‘        big business   \n",
      "4                ğŸ‘©â€ğŸ’»ğŸ¤‘        big business   \n",
      "..                ...                 ...   \n",
      "513                ğŸ‘ğŸ‘£  effective entrance   \n",
      "514                ğŸ‘ğŸªœ  effective entrance   \n",
      "515              ğŸ˜¤ğŸ—£ï¸ğŸ’¬  effective entrance   \n",
      "516                ğŸ’¨ğŸ¤¬  effective entrance   \n",
      "517              ğŸ‘ğŸ‘£â¡ï¸  effective entrance   \n",
      "\n",
      "                                               unicode  label  strategy  \\\n",
      "0                                      U+1F454 U+1F4C8      1         1   \n",
      "1                              U+1F3E2 U+1F911 U+1F911      1         1   \n",
      "2                       U+1F468 U+200D U+1F4BB U+1F91D      1         1   \n",
      "3    U+1F3E2 U+1F9D1 U+200D U+1F91D U+200D U+1F9D1 ...      1         1   \n",
      "4                       U+1F469 U+200D U+1F4BB U+1F911      1         1   \n",
      "..                                                 ...    ...       ...   \n",
      "513                                    U+1F44D U+1F463      0         6   \n",
      "514                                    U+1F44F U+1FA9C      0         6   \n",
      "515                     U+1F624 U+1F5E3 U+FE0F U+1F4AC      0         6   \n",
      "516                                    U+1F4A8 U+1F92C      0         6   \n",
      "517                      U+1F44D U+1F463 U+27A1 U+FE0F      0         6   \n",
      "\n",
      "                                           image  \\\n",
      "0      ./generated_img_dataset/test_google/0.png   \n",
      "1      ./generated_img_dataset/test_google/1.png   \n",
      "2      ./generated_img_dataset/test_google/2.png   \n",
      "3      ./generated_img_dataset/test_google/3.png   \n",
      "4      ./generated_img_dataset/test_google/4.png   \n",
      "..                                           ...   \n",
      "513  ./generated_img_dataset/test_google/513.png   \n",
      "514  ./generated_img_dataset/test_google/514.png   \n",
      "515  ./generated_img_dataset/test_google/515.png   \n",
      "516  ./generated_img_dataset/test_google/516.png   \n",
      "517  ./generated_img_dataset/test_google/517.png   \n",
      "\n",
      "                                    separate_filenames  \n",
      "0    ['noto-emoji/png/512/emoji_u1f454.png', 'noto-...  \n",
      "1    ['noto-emoji/png/512/emoji_u1f3e2.png', 'noto-...  \n",
      "2    ['noto-emoji/png/512/emoji_u1f468.png', 'noto-...  \n",
      "3    ['noto-emoji/png/512/emoji_u1f3e2.png', 'noto-...  \n",
      "4    ['noto-emoji/png/512/emoji_u1f469.png', 'noto-...  \n",
      "..                                                 ...  \n",
      "513  ['noto-emoji/png/512/emoji_u1f44d.png', 'noto-...  \n",
      "514  ['noto-emoji/png/512/emoji_u1f44f.png', 'noto-...  \n",
      "515  ['noto-emoji/png/512/emoji_u1f624.png', 'noto-...  \n",
      "516  ['noto-emoji/png/512/emoji_u1f4a8.png', 'noto-...  \n",
      "517  ['noto-emoji/png/512/emoji_u1f44d.png', 'noto-...  \n",
      "\n",
      "[518 rows x 7 columns]_only_emoji.csv\n",
      "Filtered          EM           EN                                      unicode  label  \\\n",
      "0     â±ï¸ğŸ—ºï¸ğŸ¥²  far journey         U+23F1 U+FE0F U+1F5FA U+FE0F U+1F972      1   \n",
      "1       ğŸ‘£ğŸ›£ï¸  far journey                       U+1F463 U+1F6E3 U+FE0F      1   \n",
      "2       ğŸ˜ªğŸ—ºï¸  far journey                       U+1F62A U+1F5FA U+FE0F      1   \n",
      "3    â³ğŸš—âœˆï¸ğŸ›¤ï¸  far journey  U+23F3 U+1F697 U+2708 U+FE0F U+1F6E4 U+FE0F      1   \n",
      "4     ğŸ›¤ï¸ğŸš¢ğŸ—ºï¸  far journey        U+1F6E4 U+FE0F U+1F6A2 U+1F5FA U+FE0F      1   \n",
      "..      ...          ...                                          ...    ...   \n",
      "389     ğŸƒğŸ¥¬ğŸƒ    fresh air                      U+1F343 U+1F96C U+1F343      0   \n",
      "390      ğŸ†•ğŸ‘ƒ    fresh air                              U+1F195 U+1F443      0   \n",
      "391   ğŸŒ¬ï¸ğŸ‘ƒğŸ‘ŒğŸ˜Œ    fresh air       U+1F32C U+FE0F U+1F443 U+1F44C U+1F60C      0   \n",
      "392      ğŸ˜‹ğŸ¥¬    fresh air                              U+1F60B U+1F96C      0   \n",
      "393     ğŸ§¼ğŸ‘ƒğŸ’¨    fresh air                      U+1F9FC U+1F443 U+1F4A8      0   \n",
      "\n",
      "     strategy                                       image  \\\n",
      "0           0    ./generated_img_dataset/val_google/0.png   \n",
      "1           1    ./generated_img_dataset/val_google/1.png   \n",
      "2           1    ./generated_img_dataset/val_google/2.png   \n",
      "3           0    ./generated_img_dataset/val_google/3.png   \n",
      "4           1    ./generated_img_dataset/val_google/4.png   \n",
      "..        ...                                         ...   \n",
      "389         6  ./generated_img_dataset/val_google/389.png   \n",
      "390         6  ./generated_img_dataset/val_google/390.png   \n",
      "391         6  ./generated_img_dataset/val_google/391.png   \n",
      "392         6  ./generated_img_dataset/val_google/392.png   \n",
      "393         6  ./generated_img_dataset/val_google/393.png   \n",
      "\n",
      "                                    separate_filenames  \n",
      "0    ['noto-emoji/png/512/emoji_u23f1.png', 'noto-e...  \n",
      "1    ['noto-emoji/png/512/emoji_u1f463.png', 'noto-...  \n",
      "2    ['noto-emoji/png/512/emoji_u1f62a.png', 'noto-...  \n",
      "3    ['noto-emoji/png/512/emoji_u23f3.png', 'noto-e...  \n",
      "4    ['noto-emoji/png/512/emoji_u1f6e4.png', 'noto-...  \n",
      "..                                                 ...  \n",
      "389  ['noto-emoji/png/512/emoji_u1f343.png', 'noto-...  \n",
      "390  ['noto-emoji/png/512/emoji_u1f195.png', 'noto-...  \n",
      "391  ['noto-emoji/png/512/emoji_u1f32c.png', 'noto-...  \n",
      "392  ['noto-emoji/png/512/emoji_u1f60b.png', 'noto-...  \n",
      "393  ['noto-emoji/png/512/emoji_u1f9fc.png', 'noto-...  \n",
      "\n",
      "[394 rows x 7 columns] saved as          EM           EN                                      unicode  label  \\\n",
      "0     â±ï¸ğŸ—ºï¸ğŸ¥²  far journey         U+23F1 U+FE0F U+1F5FA U+FE0F U+1F972      1   \n",
      "1       ğŸ‘£ğŸ›£ï¸  far journey                       U+1F463 U+1F6E3 U+FE0F      1   \n",
      "2       ğŸ˜ªğŸ—ºï¸  far journey                       U+1F62A U+1F5FA U+FE0F      1   \n",
      "3    â³ğŸš—âœˆï¸ğŸ›¤ï¸  far journey  U+23F3 U+1F697 U+2708 U+FE0F U+1F6E4 U+FE0F      1   \n",
      "4     ğŸ›¤ï¸ğŸš¢ğŸ—ºï¸  far journey        U+1F6E4 U+FE0F U+1F6A2 U+1F5FA U+FE0F      1   \n",
      "..      ...          ...                                          ...    ...   \n",
      "389     ğŸƒğŸ¥¬ğŸƒ    fresh air                      U+1F343 U+1F96C U+1F343      0   \n",
      "390      ğŸ†•ğŸ‘ƒ    fresh air                              U+1F195 U+1F443      0   \n",
      "391   ğŸŒ¬ï¸ğŸ‘ƒğŸ‘ŒğŸ˜Œ    fresh air       U+1F32C U+FE0F U+1F443 U+1F44C U+1F60C      0   \n",
      "392      ğŸ˜‹ğŸ¥¬    fresh air                              U+1F60B U+1F96C      0   \n",
      "393     ğŸ§¼ğŸ‘ƒğŸ’¨    fresh air                      U+1F9FC U+1F443 U+1F4A8      0   \n",
      "\n",
      "     strategy                                       image  \\\n",
      "0           0    ./generated_img_dataset/val_google/0.png   \n",
      "1           1    ./generated_img_dataset/val_google/1.png   \n",
      "2           1    ./generated_img_dataset/val_google/2.png   \n",
      "3           0    ./generated_img_dataset/val_google/3.png   \n",
      "4           1    ./generated_img_dataset/val_google/4.png   \n",
      "..        ...                                         ...   \n",
      "389         6  ./generated_img_dataset/val_google/389.png   \n",
      "390         6  ./generated_img_dataset/val_google/390.png   \n",
      "391         6  ./generated_img_dataset/val_google/391.png   \n",
      "392         6  ./generated_img_dataset/val_google/392.png   \n",
      "393         6  ./generated_img_dataset/val_google/393.png   \n",
      "\n",
      "                                    separate_filenames  \n",
      "0    ['noto-emoji/png/512/emoji_u23f1.png', 'noto-e...  \n",
      "1    ['noto-emoji/png/512/emoji_u1f463.png', 'noto-...  \n",
      "2    ['noto-emoji/png/512/emoji_u1f62a.png', 'noto-...  \n",
      "3    ['noto-emoji/png/512/emoji_u23f3.png', 'noto-e...  \n",
      "4    ['noto-emoji/png/512/emoji_u1f6e4.png', 'noto-...  \n",
      "..                                                 ...  \n",
      "389  ['noto-emoji/png/512/emoji_u1f343.png', 'noto-...  \n",
      "390  ['noto-emoji/png/512/emoji_u1f195.png', 'noto-...  \n",
      "391  ['noto-emoji/png/512/emoji_u1f32c.png', 'noto-...  \n",
      "392  ['noto-emoji/png/512/emoji_u1f60b.png', 'noto-...  \n",
      "393  ['noto-emoji/png/512/emoji_u1f9fc.png', 'noto-...  \n",
      "\n",
      "[394 rows x 7 columns]_only_emoji.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the CSV\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_val = pd.read_csv(\"val.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "# Function to check if string contains ONLY non-English characters\n",
    "def is_pure_emoji(text):\n",
    "    return not bool(re.search(r'[A-Za-z]', str(text)))\n",
    "for df in [df_train, df_test, df_val]:\n",
    "\n",
    "\n",
    "    # Keep only rows where EM is pure emoji (no English letters)\n",
    "    df_filtered = df[df['EM'].apply(is_pure_emoji)]\n",
    "\n",
    "    # Save the result\n",
    "    if df is df_train:\n",
    "        df_filtered.to_csv(\"train_only_emoji.csv\", index=False)\n",
    "    elif df is df_val:\n",
    "        df_filtered.to_csv(\"val_only_emoji.csv\", index=False)\n",
    "    elif df is df_test:\n",
    "        df_filtered.to_csv(\"test_only_emoji.csv\", index=False)\n",
    "    print(f\"Filtered {df} saved as {df}_only_emoji.csv\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPA2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
